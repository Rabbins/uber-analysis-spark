{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession, functions as F, Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación del cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levantamos el Cluster\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ClusterLocal\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "print(\"SparkContext creado con el master:\", spark.sparkContext.master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dato en un DataFrame\n",
    "ruta_fichero = \"../../data/input/dataset.csv\"\n",
    "df_spark = spark.read.csv(ruta_fichero,inferSchema=True, header=True)\n",
    "\n",
    "# Mostramos las primeras 10 columnas para ver si tiene dato y tiene sentido\n",
    "# df_spark.show(10)\n",
    "\n",
    "# Mostramos el schema inferido para ver si es coherente con el dato\n",
    "# df_spark.printSchema()\n",
    "\n",
    "# Formateo de la columna Date\n",
    "df_spark = df_spark.withColumn('Date', F.date_format(F.to_date(F.col('Date'), 'dd-MMM-yy'), 'yyyy-MM-dd'))\n",
    "\n",
    "# Generamos un índice auxiliar para poder hacer el order by y que no desmonte los datos\n",
    "\n",
    "# Para ello creamos primero la \"ventana de ordenación\"\n",
    "window_spec = Window.orderBy(F.monotonically_increasing_id())\n",
    "\n",
    "# Creamos ahora el índice en base al row_number con la ventana explicada\n",
    "df_spark = df_spark.withColumn(\"aux_index\", F.row_number().over(window_spec))\n",
    "\n",
    "# Una vez generado ese índice, vamos a ir rellenando la columna de Date con el último (last) date que no sea Null, recorriendo uno a uno\n",
    "df_spark = df_spark.withColumn(\"Date\", F.last(\"Date\", ignorenulls=True).over(window_spec))\n",
    "\n",
    "df_spark = df_spark.drop(\"aux_index\")\n",
    "\n",
    "# # Validamos\n",
    "df_spark.show(30)\n",
    "\n",
    "# Limpiamos los nombres de las columnas que tienen espacios\n",
    "df_spark = df_spark.select([F.col(col).alias(col.strip()) for col in df_spark.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Which date had the most completed trips during the two-week period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_by_day = df_spark.groupBy(\"Date\").agg(F.sum(\"Completed Trips\").alias(\"total_trips\"))\n",
    "df_trips_by_day.orderBy(\"total_trips\",ascending=False).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What was the highest number of completed trips within a 24-hour period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una columna auxiliar de Date + Time en formato Unix (para poder hacer luego la resta de segundos para generar una ventana de 24 horas)\n",
    "df_spark = df_spark.withColumn(\n",
    "    \"DateTime_unix\",\n",
    "    F.to_timestamp(F.concat(F.col(\"Date\"), F.lit(\" \"), F.col(\"Time (Local)\")), \"dd/MM/yyyy H\").cast(\"long\"))\n",
    "\n",
    "# Definir la ventana de tiempo de las últimas 24 horas\n",
    "window_spec = Window.orderBy(\"DateTime_unix\").rangeBetween(-86400, 0)  # 86400 segundos = 24 horas\n",
    "\n",
    "# Calcular la suma de \"Completed Trips\" en una ventana de 24 horas\n",
    "df_trips_by_24h_period = df_spark.withColumn(\"sum_last_24_hours\", F.sum(\"Completed Trips\").over(window_spec))\n",
    "\n",
    "# Extraer el máximo y mostrarlo por pantalla\n",
    "df_trips_by_24h_period.select(F.max(df_trips_by_24h_period.sum_last_24_hours)).show()\n",
    "\n",
    "# Asignar el valor a una variable para poder mostrar luego el registro donde el sum de horas sea = al valor\n",
    "max_value = df_trips_by_24h_period.agg(F.max(\"sum_last_24_hours\").alias(\"max_trips_in_24h\")).collect()[0][\"max_trips_in_24h\"]\n",
    "\n",
    "df_trips_by_24h_period.filter(F.col(\"sum_last_24_hours\") == max_value).show()\n",
    "df_trips_by_24h_period.filter(df_trips_by_24h_period.sum_last_24_hours == max_value).show()\n",
    "\n",
    "# Eliminamos la columna auxiliar que hemos creado\n",
    "df_spark = df_spark.drop(\"DateTime_unix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Which hour of the day had the most requests during the two-week period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe con el sum de requests por hour\n",
    "df_requests_per_hour = df_spark.groupBy(\"Time (Local)\").agg(F.sum(\"Requests\").alias(\"requests_per_hour\"))\n",
    "\n",
    "# Valor máximo\n",
    "max_requests_hour = df_requests_per_hour.agg(F.max(\"requests_per_hour\").alias(\"max_requests_hour\")).collect()[0][\"max_requests_hour\"]\n",
    "\n",
    "df_requests_per_hour.filter(F.col(\"requests_per_hour\") == max_requests_hour).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What percentages of all zeroes during the two-week period occurred on weekend (Friday at 5 pm to Sunday at 3 am)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip: The local time value is the start of the hour (e.g. 15 is the hour from 3:00 pm - 4:00 pm)\n",
    "\n",
    "# Columna is_weekend\n",
    "df_spark = df_spark.withColumn(\n",
    "    'is_weekend',\n",
    "    F.when(\n",
    "        (F.dayofweek('Date').isin(1, 7)) |  # Caso 1: Domingo o sábado\n",
    "        ((F.dayofweek('Date') == 6) & (F.col('Time (Local)') >= 17)) |\n",
    "        ((F.dayofweek('Date') == 2) & (F.col('Time (Local)') <= 3)),  # Caso 2: Viernes y hora >= 17\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "df_spark = df_spark.repartition(8)\n",
    "\n",
    "total_zeroes = df_spark.agg(F.sum(\"Zeroes\").alias(\"total_zeroes\")).collect()[0][\"total_zeroes\"]\n",
    "weekend_zeroes = df_spark \\\n",
    "    .filter(F.col(\"is_weekend\")==True) \\\n",
    "    .agg(F.sum(\"Zeroes\").alias(\"weekend_zeroes\")) \\\n",
    "    .collect()[0][\"weekend_zeroes\"]\n",
    "\n",
    "percentage_of_weekend_zeroes = (weekend_zeroes / total_zeroes) * 100\n",
    "\n",
    "percentage_of_weekend_zeroes_str = f\"{percentage_of_weekend_zeroes:.2f}%\"\n",
    "\n",
    "print(percentage_of_weekend_zeroes_str)\n",
    "\n",
    "df_spark = df_spark.drop('is_weekend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What is the weighted average ratio of completed trips per driver during the two-week period? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip: “Weighted average” means your answer should account for the total trip volume in each hour to determine the most accurate number in the whole period.\n",
    "weighted_avg = df_spark.withColumn(\"completed_trips_per_driver\", df_spark[\"Completed Trips\"] / df_spark[\"Unique Drivers\"]) \\\n",
    "                 .groupBy(\"Date\", \"Time (Local)\") \\\n",
    "                 .agg(\n",
    "                    F.avg(\"completed_trips_per_driver\").alias(\"avg_completed_per_driver\"),\n",
    "                    F.sum(\"Completed Trips\").alias(\"total_completed_trips\")\n",
    "                 ) \\\n",
    "                 .withColumn(\"weighted_ratio\", F.col(\"avg_completed_per_driver\") * F.col(\"total_completed_trips\")) \\\n",
    "                 .agg(F.sum(\"weighted_ratio\") / F.sum(\"total_completed_trips\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. In drafting a driver schedule in terms of 8 hours shifts, when are the busiest 8 consecutive hours over the two-week period in terms of unique requests? A new shift starts every 8 hours. Assume that a driver will work the same shift each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una sola línea para calcular el bloque de 8 horas con más Requests\n",
    "busiest_8_hours = (df_spark\n",
    "    .withColumn(\"timestamp\", F.concat(F.col(\"Date\"), F.lit(\" \"), F.col(\"Time (Local)\")))\n",
    "    .withColumn(\"consecutive_requests\", F.sum(\"Requests\").over(Window.orderBy(\"timestamp\").rowsBetween(0, 7)))\n",
    "    .orderBy(F.desc(\"consecutive_requests\"))\n",
    "    .limit(1)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True or False: Driver supply always increases when demand increases during the two-week period. Tip: Visualize the data to confirm your answer if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una ventana ordenada por \"Date\" y \"Time (Local)\"\n",
    "window = Window.orderBy(\"Date\", \"Time (Local)\")\n",
    "\n",
    "driver_supply_increase = (df_spark\n",
    "    .withColumn(\"difference_requests\", F.col(\"Requests\")-F.lag(\"Requests\",1).over(window))\n",
    "    .withColumn(\"difference_drivers\", F.col(\"Unique Drivers\")-F.lag(\"Unique Drivers\",1).over(window))\n",
    "    .filter(F.col(\"difference_requests\")>0)\n",
    "    .withColumn(\"condition_met\", \n",
    "                F.when((F.col(\"difference_requests\") >= 0) & (F.col(\"difference_drivers\") > 0), True)\n",
    "                .otherwise(False))\n",
    "    .select(F.min((\"condition_met\")))    \n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. In which 72-hour period is the ratio of Zeroes to Eyeballs the highest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.orderBy(\"timestamp\").rangeBetween(-72 * 60 * 60 * 1000, 0)\n",
    "\n",
    "zeroes_to_eyeballs = (df_spark\n",
    "    .withColumn(\"timestamp\", F.to_timestamp(F.concat(F.col(\"Date\"), F.lit(\" \"), F.col(\"Time (Local)\")),\"yyyy-MM-dd H\").cast(\"long\") * 1000)\n",
    "    .withColumn(\"zeroes_last72h\", F.sum(\"Zeroes\").over(window))\n",
    "    .withColumn(\"eyeballs_last72h\", F.sum(\"Eyeballs\").over(window))\n",
    "    .withColumn(\"highest_ratio\", F.col(\"zeroes_last72h\")/F.col(\"eyeballs_last72h\"))\n",
    "    .select(F.max('highest_ratio'))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. If you could add 5 drivers to any single hour of every day during the two-week period, which hour should you add them to? Hint: Consider both rider eyeballs and driver supply when choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiene sentido sumar Riders (que van a hacer varios viajes por periodo) a aquellas horas que menos % de servicio tengan y quizá, que más número de Eyeballs tengan.\n",
    "# No vale quedarse con 4 eyeballs y 1 rider y veas un 1/4 que un 400 eyeballs y 100 riders. Hay mucho margen de ganancia\n",
    "# Así que miraría de los que mayor porcentaje tengan, los que tengan mayor número de eyeballs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "\n",
    "service_provided = df_spark \\\n",
    "    .withColumn(\"availability_of_service\", F.col(\"Unique Drivers\")/F.col(\"Eyeballs\")) \\\n",
    "    .groupBy(\"Time (Local)\") \\\n",
    "    .agg(\n",
    "        F.sum(\"Eyeballs\").alias(\"usuarios_unicos_loggeados\"),\n",
    "        F.sum(\"Zeroes\").alias(\"usuarios_sin_coche\"),\n",
    "        F.avg(\"availability_of_service\").alias(\"media_de_servicio\")\n",
    "    )\\\n",
    "    .withColumn(\"priority_score\", \n",
    "                (1 / F.col(\"media_de_servicio\")) * F.col(\"usuarios_unicos_loggeados\") * (1 + alpha * F.col(\"usuarios_sin_coche\")) # Fórmula para calcular el impacto de mayor oferta\n",
    "               ) \\\n",
    "    .orderBy(\"priority_score\", ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Looking at the data from all two weeks, which time might make the most sense to consider a true “end day” instead of midnight? (i.e when are supply and demand at both their natural minimums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voy a buscar aquella hora con menor número de Eyeballs y Drivers\n",
    "minimum_demand_offer = df_spark \\\n",
    "    .groupBy(\"Time (Local)\") \\\n",
    "    .agg(\n",
    "        F.avg(\"Unique Drivers\").alias(\"drivers\"),\n",
    "        F.avg(\"Eyeballs\").alias(\"eyeballs\")\n",
    "    ) \\\n",
    "    .orderBy(F.col(\"drivers\").asc(), F.col(\"eyeballs\").asc()) \\\n",
    "    .show()\n",
    "    \n",
    "# Me quedo con el que tenga el menor número de drivers puesto que directamente no habrá opción de negocio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
